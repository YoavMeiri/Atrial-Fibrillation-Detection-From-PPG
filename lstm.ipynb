{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/meiri.yoav/biomed_proj/pyPPG')\n",
    "\n",
    "from Prefiltering import*\n",
    "from FiducialPoints import*\n",
    "from Biomarkers2 import*\n",
    "from Summary import*\n",
    "from Statistics import*\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import seaborn as sns\n",
    "# set seed 42\n",
    "import sys\n",
    "sys.path.append('/home/meiri.yoav/biomed_proj/pyPPG')\n",
    "\n",
    "from Prefiltering import*\n",
    "from FiducialPoints import*\n",
    "from Biomarkers2 import*\n",
    "from Summary import*\n",
    "from Statistics import*\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, roc_curve, auc, confusion_matrix, classification_report\n",
    "# set seed 42\n",
    "np.random.seed(42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup:\n",
    "\n",
    "Input: 10 ppg cycles (each one represented as 1x30 feature vector, total 10x30 matrix)\n",
    "\n",
    "Output: 0/1 (0: non_af, 1: af)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_series(X, B, series_size):\n",
    "    n = X.shape[0]\n",
    "    idx = np.random.randint(0, n-series_size, B)\n",
    "    onsets = np.zeros(B)\n",
    "    X_sampled = np.zeros((B, series_size, X.shape[1]-3)) # substracting 3 because we ommit Tpi, sample_idx and os\n",
    "    for i in range(B):\n",
    "        X_sampled[i,:,:] = X.iloc[idx[i]:idx[i]+series_size,:].drop(columns=['Tpi', 'sample_idx', 'os'])\n",
    "        # put in onsets[i] the onset value of the record in idx[i]\n",
    "        onsets[i] = X.iloc[idx[i],:].os\n",
    "        \n",
    "    return X_sampled, onsets, idx\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this setup we train on sampled 30 cycle long series and test & validate on non-sampled (sliced) 30 cycle long series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def excract_hrv_features(cycle_signal_30, fs):\n",
    "    from hrvanalysis.extract_features import get_time_domain_features\n",
    "    from hrvanalysis.extract_features import get_frequency_domain_features\n",
    "    from hrvanalysis.extract_features import get_geometrical_features\n",
    "    from hrvanalysis.extract_features import get_poincare_plot_features\n",
    "    from hrvanalysis.extract_features import get_csi_cvi_features\n",
    "    import pandas as pd\n",
    "\n",
    "    ''' reference: https://aura-healthcare.github.io/hrv-analysis/hrvanalysis.html'''\n",
    "\n",
    "    time_domain_features = get_time_domain_features(cycle_signal_30)\n",
    "    frequency_domain_features = get_frequency_domain_features(cycle_signal_30, sampling_frequency=fs)\n",
    "    # geometrical_features = get_geometrical_features(cycle_signal_30)\n",
    "    poincare_plot_features = get_poincare_plot_features(cycle_signal_30)\n",
    "    csi_cvi_features = get_csi_cvi_features(cycle_signal_30)\n",
    "\n",
    "    # aggregate all features into one dict and turn it into a dataframe\n",
    "    features = {\n",
    "                **time_domain_features,\n",
    "                **frequency_domain_features,\n",
    "                # **geometrical_features,\n",
    "                **poincare_plot_features,\n",
    "                **csi_cvi_features\n",
    "                }\n",
    "    features_arr = pd.DataFrame(features, index=[0]).to_numpy()\n",
    "    return features_arr.squeeze()\n",
    "\n",
    "def get_outlier_bounds(arr):\n",
    "    q1 = np.quantile(arr, 0.25)\n",
    "    q3 = np.quantile(arr, 0.75)\n",
    "    iqr = q3-q1\n",
    "    lower_bound = q1 - 1.5*iqr\n",
    "    upper_bound = q3 + 1.5*iqr\n",
    "    return lower_bound, upper_bound\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.impute import KNNImputer\n",
    "from hrvanalysis.preprocessing import get_nn_intervals\n",
    "\n",
    "# B = 100# 1000 10 cycle long samples for each subject\n",
    "# series_size = 30\n",
    "# fs = 125\n",
    "# add_hrv = True\n",
    "# add_morph = True\n",
    "\n",
    "\n",
    "# annot_files = sorted(glob.glob(\"/home/meiri.yoav/biomed_proj/data/annotated/*\"))\n",
    "# af_files = [f for f in annot_files if 'non' not in f]\n",
    "# non_af_files = [f for f in annot_files if 'non' in f]\n",
    "\n",
    "# names = [tuple(f.split('/')[-1].split('.')[0].split('_')[1:]) for f in annot_files]\n",
    "\n",
    "\n",
    "# non_af_subs = [t  for t in list(names) if 'non' in t]\n",
    "# af_subs = [t  for t in list(names) if 'non' not in t]\n",
    "\n",
    "# train_subs = non_af_subs[:int(len(non_af_subs)*0.8)] + af_subs[:int(len(af_subs)*0.8)]\n",
    "# test_subs = non_af_subs[int(len(non_af_subs)*0.8):] + af_subs[int(len(af_subs)*0.8):]\n",
    "\n",
    "def exctract_sub_features(train_subs, annot_files, add_hrv=True, add_morph=True, B=100, series_size=30, fs=125):\n",
    "    \"\"\" exctract features from all subjects in train_subs \n",
    "\n",
    "    Args:\n",
    "        train_subs (list): A list of train subject names to extract features from (the rest will be used for testing)\n",
    "        annot_files (list): A list of all annotation files\n",
    "        add_hrv (bool, optional): Defaults to True.\n",
    "        add_morph (bool, optional): Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of all features extracted from all subjects in names\n",
    "    \"\"\"\n",
    "    # print how much af and non af subjects we have in train and test sets\n",
    "    # print(f'af subjects in train set: {len([s for s in train_subs if \"non\" not in s])}')\n",
    "    # print(f'non af subjects in train set: {len([s for s in train_subs if \"non\" in s])}')\n",
    "    # print(f'af subjects in test set: {len([s for s in test_subs if \"non\" not in s])}')\n",
    "    # print(f'non af subjects in test set: {len([s for s in test_subs if \"non\" in s])}')\n",
    "\n",
    "    subs_features = {}\n",
    "    for f in annot_files:\n",
    "        sub_name = tuple(f.split('/')[-1].split('.')[0].split('_')[1:])\n",
    "        # print(f'processing {sub_name}')\n",
    "        subs_features[sub_name] = {}\n",
    "        with open(f, 'rb') as f:\n",
    "            annot = pkl.load(f)\n",
    "\n",
    "        X = annot['osignal_data']\n",
    "\n",
    "        if sub_name in train_subs:\n",
    "            first_index_of_X = int(len(X)*0.1)\n",
    "            test_subset = X.iloc[0:first_index_of_X,:] # take the first 20% of the data as test subset\n",
    "            \n",
    "            \n",
    "            # remove test subset from X\n",
    "            X = X.drop(test_subset.index)\n",
    "            \n",
    "            \n",
    "            signal = annot['hr']\n",
    "            peaks = annot['fiducials']['pk']\n",
    "            # print(f'len of peaks: {len(peaks)}')\n",
    "            #? Setup test and validation sets\n",
    "            test_series = np.zeros((int(len(test_subset)/series_size), series_size, test_subset.shape[1]-3)) # substracting 3 because we ommit Tpi, sample_idx and os\n",
    "            hrv_features = []\n",
    "            for i in range(int(len(test_subset)/series_size)):\n",
    "                if add_hrv:\n",
    "                    right_bound = min((i+1)*series_size+1, len(peaks))\n",
    "                    nn_intervals = np.diff(peaks[i*series_size : right_bound])\n",
    "                    # print(i, ': ', nn_intervals.shape, excract_hrv_features(nn_intervals, fs).shape)\n",
    "                    hrv_features.append(excract_hrv_features(nn_intervals, fs))\n",
    "                \n",
    "                if add_morph:\n",
    "                    test_series[i,:,:] = test_subset.iloc[i*series_size:(i+1)*series_size,:].drop(columns=['Tpi', 'sample_idx', 'os'])\n",
    "            \n",
    "            # print(np.array(hrv_features).shape)\n",
    "            if add_hrv:\n",
    "                # add hrv features to test_series\n",
    "                hrv_features = np.array([hrv_features for i in range(series_size)]).reshape((len(hrv_features), series_size, hrv_features[0].shape[0]))\n",
    "                test_series = np.concatenate((test_series, hrv_features), axis=2)\n",
    "            \n",
    "            subs_features[sub_name]['val_split'] = test_series\n",
    "            # subs_features[sub_name]['test_split'] = test_series[int(len(test_series)/2):]\n",
    "            \n",
    "            #? Setup train set\n",
    "            \n",
    "            data, onsets, idx = sample_series(X, B, series_size)\n",
    "\n",
    "            if not add_morph:\n",
    "                # make data an array of zeros of the same shape as data\n",
    "                data = np.zeros(data.shape)\n",
    "            # print(data.shape)\n",
    "            if add_hrv:\n",
    "                hrv_features = []\n",
    "                for i in range(B):\n",
    "                    right_bound = min(first_index_of_X+idx[i]+series_size+1, len(peaks))\n",
    "                    nn_intervals = np.diff(peaks[first_index_of_X+idx[i] : right_bound])\n",
    "                    hrv_features.append(excract_hrv_features(nn_intervals, fs))\n",
    "                    \n",
    "                hrv_features = np.array([hrv_features for i in range(series_size)]).reshape((len(hrv_features), series_size, hrv_features[0].shape[0]))\n",
    "                data = np.concatenate((data, hrv_features), axis=2)\n",
    "            \n",
    "            \n",
    "            subs_features[sub_name]['train_split'] = data\n",
    "        else:\n",
    "            # print('test_sub')\n",
    "            # In this case just split to 30 cycle long series and pad with zeros if needed\n",
    "            data = np.zeros((int(len(X)/series_size), series_size, X.shape[1]-3))\n",
    "            # test_onsets = []\n",
    "            hrv_features = []\n",
    "            for i in range(int(len(X)/series_size)):\n",
    "                if add_hrv:\n",
    "                    right_bound = min((i+1)*series_size+1, len(peaks))\n",
    "                    if right_bound <= i*series_size:\n",
    "                        continue\n",
    "                    # print((i*series_size, right_bound))\n",
    "                    nn_intervals = np.diff(peaks[i*series_size : right_bound])\n",
    "                    if len(nn_intervals) == 0:\n",
    "                        continue\n",
    "                    hrv_features.append(excract_hrv_features(nn_intervals, fs))\n",
    "                if add_morph:\n",
    "                    data[i,:,:] = X.iloc[i*series_size:(i+1)*series_size,:].drop(columns=['Tpi', 'sample_idx', 'os'])\n",
    "            \n",
    "            if add_hrv:\n",
    "                # add hrv features to test_series\n",
    "                hrv_features = np.array([hrv_features for i in range(series_size)]).reshape((len(hrv_features), series_size, hrv_features[0].shape[0]))\n",
    "                # print(hrv_features.shape)\n",
    "                data = np.concatenate((data[:len(hrv_features),:,:], hrv_features), axis=2)\n",
    "            \n",
    "            subs_features[sub_name]['test_split'] = data\n",
    "    \n",
    "    return subs_features\n",
    "            \n",
    "# subs_features = exctract_sub_features(train_subs, annot_files, add_hrv=True, add_morph=True)\n",
    "# # print the name and the shapes of all subs in subs_features, and consider if it's a train sub or a test sub\n",
    "# for sub in subs_features.keys():\n",
    "#     if sub in train_subs:\n",
    "#         print(sub, subs_features[sub]['train_split'].shape, subs_features[sub]['val_split'].shape, subs_features[sub]['test_split'].shape)\n",
    "#     else:\n",
    "#         print(sub, subs_features[sub]['val_split'].shape, subs_features[sub]['test_split'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(sub_features, train_subs, test_subs):\n",
    "    # devide to train and test subjects where both train and test groups contain all the classes\n",
    "    \n",
    "    \n",
    "    X_train = np.concatenate([sub_features[t]['train_split'] for t in train_subs], axis=0) # amoung the train subjects, take 80% of each subject to train set\n",
    "    y_train = np.concatenate([np.ones(int(sub_features[t]['train_split'].shape[0]))*int('non' not in t) for t in train_subs], axis=0)\n",
    "\n",
    "    X_val = np.concatenate([sub_features[t]['val_split'] for t in train_subs], axis=0)\n",
    "    y_val = np.concatenate([np.ones(int(sub_features[t]['val_split'].shape[0]))*int('non' not in t) for t in train_subs], axis=0)\n",
    "    \n",
    "    X_test = np.concatenate([sub_features[t]['test_split'] for t in test_subs], axis=0)\n",
    "    y_test = np.concatenate([np.ones(int(sub_features[t]['test_split'].shape[0]))*int('non' not in t) for t in test_subs], axis=0)\n",
    "    \n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, train_subs, test_subs\n",
    "    \n",
    "\n",
    "    \n",
    "# X_train, y_train, X_val_from_test_subs, y_val_from_test_subs, X_test_from_test_subs, y_test_from_test_subs, train_subs, test_subs = create_dataset(subs_features, train_subs, test_subs)\n",
    "# # print the shapes of the data\n",
    "# print('X_train shape: ', X_train.shape)\n",
    "# print('y_train shape: ', y_train.shape)\n",
    "# print('X_val_from_test_subs shape: ', X_val_from_test_subs.shape)\n",
    "# print('y_val_from_test_subs shape: ', y_val_from_test_subs.shape)\n",
    "# print('X_test_from_test_subs shape: ', X_test_from_test_subs.shape)\n",
    "# print('y_test_from_test_subs shape: ', y_test_from_test_subs.shape)\n",
    "\n",
    "# print('train_subs: ', train_subs)\n",
    "# print('test_subs: ', test_subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all data on 2d using pca and color the data by the labels\n",
    "def plot_pca(X_train, X_val, X_test, y_train, y_val, y_test, train_subs, test_subs):\n",
    "    pca = PCA(n_components=2)\n",
    "\n",
    "    # create one big stacked X and y\n",
    "    X = np.concatenate([X_train, X_val, X_test], axis=0)\n",
    "    # fit transform X and devide it to the original sets\n",
    "    X_pca = pca.fit_transform(X.reshape((X.shape[0], X.shape[1]*X.shape[2])))\n",
    "    X_train_pca = X_pca[:X_train.shape[0]]\n",
    "    X_val_pca = X_pca[X_train.shape[0]:X_train.shape[0]+X_val.shape[0]]\n",
    "    X_test_pca = X_pca[X_train.shape[0]+X_val.shape[0]:] \n",
    "\n",
    "\n",
    "    # plot the data colors are according to labels and shapes are according to the set (train, val, test)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.scatter(X_train_pca[:,0], X_train_pca[:,1], c=y_train, marker='o', label='train')\n",
    "    plt.scatter(X_val_pca[:,0], X_val_pca[:,1], c=y_val, marker='*', label='val')\n",
    "    plt.scatter(X_test_pca[:,0], X_test_pca[:,1], c=y_test, marker='x', label='test')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmeiri-yoav\u001b[0m (\u001b[33myoavmeiri\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.15.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20230711_183254-t7s4ec50</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yoavmeiri/lightning_logs/runs/t7s4ec50' target=\"_blank\">solar-firefly-12</a></strong> to <a href='https://wandb.ai/yoavmeiri/lightning_logs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yoavmeiri/lightning_logs' target=\"_blank\">https://wandb.ai/yoavmeiri/lightning_logs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yoavmeiri/lightning_logs/runs/t7s4ec50' target=\"_blank\">https://wandb.ai/yoavmeiri/lightning_logs/runs/t7s4ec50</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a dataset and model using pytorch lightning module and biderectional lstm for this task\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import classification_report\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "import torchmetrics\n",
    "\n",
    "wandb_logger = WandbLogger()\n",
    "\n",
    "\n",
    "class afPPGDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y\n",
    "        self.len = len(y)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "class afPPGModel(pl.LightningModule):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, learning_rate, X_train, y_train, X_val, y_val, X_test, y_test, feature_set_type_name):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "        # Add 2 layer fully-connected with ReLu in the middle\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size*2, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_classes),\n",
    "            nn.Softmax())\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.f1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.auc = torchmetrics.AUROC(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        \n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._log_stats(batch,  name_template='train')\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self._log_stats(batch,  name_template='val')\n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss = self._log_stats(batch,  name_template='test')\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        train_dataset = afPPGDataset(self.X_train, self.y_train)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        return train_loader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        val_dataset = afPPGDataset(self.X_val, self.y_val)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "        return val_loader\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        test_dataset = afPPGDataset(self.X_test, self.y_test)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "        return test_loader\n",
    "    \n",
    "    def _log_stats(self, batch, name_template):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        loss = F.cross_entropy(logits, y.long())\n",
    "        f1 = self.f1(preds, y)\n",
    "        acc = self.accuracy(preds, y)\n",
    "        # auc = self.auc(preds, y)\n",
    "        self.log(name_template + '_loss', loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(name_template + '_acc', acc, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(name_template + '_f1', f1, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def predict(self, x):\n",
    "        y_hat = self(x)\n",
    "        return y_hat\n",
    "    \n",
    "    def predict_proba(self, x):\n",
    "        y_hat = self(x)\n",
    "        return F.softmax(y_hat, dim=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "B = 100# 1000 10 cycle long samples for each subject\n",
    "series_size = 30\n",
    "fs = 125\n",
    "add_hrv = True\n",
    "add_morph = True\n",
    "\n",
    "\n",
    "annot_files = sorted(glob.glob(\"/home/meiri.yoav/biomed_proj/data/annotated/*\"))\n",
    "af_files = [f for f in annot_files if 'non' not in f]\n",
    "non_af_files = [f for f in annot_files if 'non' in f]\n",
    "\n",
    "names = [tuple(f.split('/')[-1].split('.')[0].split('_')[1:]) for f in annot_files]\n",
    "\n",
    "\n",
    "non_af_subs = [t  for t in list(names) if 'non' in t]\n",
    "af_subs = [t  for t in list(names) if 'non' not in t]\n",
    "\n",
    "\n",
    "different_train_test_split_num = 50\n",
    "classification_resaults_arr = []\n",
    "for train_split_config in range(different_train_test_split_num):\n",
    "    try:\n",
    "        # sample an array of indexes from 0 to len(non_af_subs) - 1, and take the first 80% of the indexes for the train subs and the last 20% for the test subs\n",
    "        non_af_train_subs_idxs = np.random.choice(len(non_af_subs), int(len(non_af_subs)*0.8), replace=False)\n",
    "        af_train_subs_idxs = np.random.choice(len(af_subs), int(len(af_subs)*0.8), replace=False)\n",
    "        train_subs = [non_af_subs[i] for i in non_af_train_subs_idxs] + [af_subs[i] for i in af_train_subs_idxs]\n",
    "\n",
    "        test_subs = [t for t in list(names) if t not in train_subs]\n",
    "        # print train subs and test subs in the same line\n",
    "        print('train subs: ' + str(train_subs) + ' test subs: ' + str(test_subs))\n",
    "        subs_features = exctract_sub_features(train_subs, annot_files, add_hrv=add_hrv, add_morph=add_morph, B=B, series_size=series_size, fs=fs)\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test, train_subs, test_subs = create_dataset(subs_features, train_subs, test_subs)\n",
    "\n",
    "        # print the shapes of all data splits\n",
    "        print('X_train shape: ' + str(X_train.shape))\n",
    "        print('y_train shape: ' + str(y_train.shape))\n",
    "        print('X_val shape: ' + str(X_val.shape))\n",
    "        print('y_val shape: ' + str(y_val.shape))\n",
    "        print('X_test shape: ' + str(X_test.shape))\n",
    "        print('y_test shape: ' + str(y_test.shape))\n",
    "        feature_set_type_name = \"hrv + morph\"\n",
    "        user = \"YoavMeiri\"\n",
    "        project = \"bioML - AF detection from PPG - lstm - for report\"\n",
    "        name = f\"{feature_set_type_name} run {train_split_config}\"\n",
    "        wandb.init(entity=user, project=project, name=name)\n",
    "\n",
    "        print('X_train shape: ' + str(X_train.shape))\n",
    "        if add_hrv:\n",
    "            input_dim = 59\n",
    "        else:\n",
    "            input_dim = 30\n",
    "        # plot_pca(X_train, X_val_from_test_subs, X_test_from_test_subs, y_train, y_val_from_test_subs, y_test_from_test_subs)\n",
    "        # create a model\n",
    "        model = afPPGModel(input_size=input_dim, hidden_size=64, num_layers=2, num_classes=2, learning_rate=0.0001,\n",
    "                        X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val,\n",
    "                        X_test=X_test, y_test=y_test, feature_set_type_name=feature_set_type_name)\n",
    "\n",
    "        # create a trainer\n",
    "        # trainer = pl.Trainer(max_epochs=15)\n",
    "        # logger = CSVLogger(\"logs\", name=\"my_exp_name\")\n",
    "        logger = WandbLogger()\n",
    "        print('Creating trainer...')\n",
    "        trainer = pl.Trainer(max_epochs=10, num_sanity_val_steps=2, logger=logger)\n",
    "        \n",
    "        print('validating initial model...')\n",
    "        trainer.validate(model)\n",
    "        # train the model\n",
    "        print('training model...')\n",
    "        trainer.fit(model)\n",
    "\n",
    "        # test the model\n",
    "        print('testing model...')\n",
    "        trainer.test(model)\n",
    "        \n",
    "        wandb.finish()\n",
    "                # metrics, results = plot_classification_results(X_train, y_train, X_val_from_test_subs, y_val_from_test_subs, X_test_from_test_subs, y_test_from_test_subs)\n",
    "        # classification_resaults_arr.append(results)\n",
    "    except:\n",
    "        print('failed at: ' + str(train_split_config))\n",
    "        train_split_config -= 1\n",
    "\n",
    "wandb.finish()\n",
    "# print(\"total number of train test splits: \" + str(len(classification_resaults_arr)))\n",
    "# train_subs = non_af_subs[:int(len(non_af_subs)*0.8)] + af_subs[:int(len(af_subs)*0.8)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
