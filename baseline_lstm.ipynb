{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/meiri.yoav/biomed_proj/pyPPG')\n",
    "\n",
    "from Prefiltering import*\n",
    "from FiducialPoints import*\n",
    "from Biomarkers2 import*\n",
    "from Summary import*\n",
    "from Statistics import*\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import seaborn as sns\n",
    "# set seed 42\n",
    "np.random.seed(42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup:\n",
    "\n",
    "Input: 10 ppg cycles (each one represented as 1x30 feature vector, total 10x30 matrix)\n",
    "\n",
    "Output: 0/1 (0: non_af, 1: af)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_series(X, B, series_size):\n",
    "    n = X.shape[0]\n",
    "    idx = np.random.randint(0, n-series_size, B)\n",
    "    onsets = np.zeros(B)\n",
    "    X_sampled = np.zeros((B, series_size, X.shape[1]-3)) # substracting 3 because we ommit Tpi, sample_idx and os\n",
    "    for i in range(B):\n",
    "        X_sampled[i,:,:] = X.iloc[idx[i]:idx[i]+series_size,:].drop(columns=['Tpi', 'sample_idx', 'os'])\n",
    "        # put in onsets[i] the onset value of the record in idx[i]\n",
    "        onsets[i] = X.iloc[idx[i],:].os\n",
    "        \n",
    "    return X_sampled, onsets, idx\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this setup we train on sampled 30 cycle long series and test & validate on non-sampled (sliced) 30 cycle long series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def excract_hrv_features(cycle_signal_30, fs):\n",
    "    from hrvanalysis.extract_features import get_time_domain_features\n",
    "    from hrvanalysis.extract_features import get_frequency_domain_features\n",
    "    from hrvanalysis.extract_features import get_geometrical_features\n",
    "    from hrvanalysis.extract_features import get_poincare_plot_features\n",
    "    from hrvanalysis.extract_features import get_csi_cvi_features\n",
    "    import pandas as pd\n",
    "\n",
    "    ''' reference: https://aura-healthcare.github.io/hrv-analysis/hrvanalysis.html'''\n",
    "\n",
    "    time_domain_features = get_time_domain_features(cycle_signal_30)\n",
    "    frequency_domain_features = get_frequency_domain_features(cycle_signal_30, sampling_frequency=fs)\n",
    "    # geometrical_features = get_geometrical_features(cycle_signal_30)\n",
    "    poincare_plot_features = get_poincare_plot_features(cycle_signal_30)\n",
    "    csi_cvi_features = get_csi_cvi_features(cycle_signal_30)\n",
    "\n",
    "    # aggregate all features into one dict and turn it into a dataframe\n",
    "    features = {\n",
    "                **time_domain_features,\n",
    "                **frequency_domain_features,\n",
    "                # **geometrical_features,\n",
    "                **poincare_plot_features,\n",
    "                **csi_cvi_features\n",
    "                }\n",
    "    features_arr = pd.DataFrame(features, index=[0]).to_numpy()\n",
    "    return features_arr.squeeze()\n",
    "\n",
    "def get_outlier_bounds(arr):\n",
    "    q1 = np.quantile(arr, 0.25)\n",
    "    q3 = np.quantile(arr, 0.75)\n",
    "    iqr = q3-q1\n",
    "    lower_bound = q1 - 1.5*iqr\n",
    "    upper_bound = q3 + 1.5*iqr\n",
    "    return lower_bound, upper_bound\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "af subjects in train set: 14\n",
      "non af subjects in train set: 12\n",
      "af subjects in test set: 4\n",
      "non af subjects in test set: 3\n",
      "('af', '001') (100, 30, 30) (5, 30, 30) (6, 30, 30)\n",
      "('af', '002') (100, 30, 30) (4, 30, 30) (5, 30, 30)\n",
      "('af', '003') (100, 30, 30) (4, 30, 30) (5, 30, 30)\n",
      "('af', '005') (100, 30, 30) (4, 30, 30) (4, 30, 30)\n",
      "('af', '006') (100, 30, 30) (5, 30, 30) (5, 30, 30)\n",
      "('af', '007') (100, 30, 30) (5, 30, 30) (5, 30, 30)\n",
      "('af', '008') (100, 30, 30) (5, 30, 30) (5, 30, 30)\n",
      "('af', '009') (100, 30, 30) (5, 30, 30) (6, 30, 30)\n",
      "('af', '010') (100, 30, 30) (5, 30, 30) (5, 30, 30)\n",
      "('af', '011') (100, 30, 30) (5, 30, 30) (6, 30, 30)\n",
      "('af', '012') (100, 30, 30) (5, 30, 30) (5, 30, 30)\n",
      "('af', '013') (100, 30, 30) (5, 30, 30) (5, 30, 30)\n",
      "('af', '014') (100, 30, 30) (5, 30, 30) (6, 30, 30)\n",
      "('af', '015') (100, 30, 30) (4, 30, 30) (5, 30, 30)\n",
      "('af', '016') (29, 30, 30) (29, 30, 30)\n",
      "('af', '017') (24, 30, 30) (24, 30, 30)\n",
      "('af', '018') (29, 30, 30) (30, 30, 30)\n",
      "('af', '019') (27, 30, 30) (28, 30, 30)\n",
      "('non', 'af', '001') (100, 30, 30) (3, 30, 30) (4, 30, 30)\n",
      "('non', 'af', '002') (100, 30, 30) (3, 30, 30) (3, 30, 30)\n",
      "('non', 'af', '003') (100, 30, 30) (6, 30, 30) (6, 30, 30)\n",
      "('non', 'af', '005') (100, 30, 30) (7, 30, 30) (7, 30, 30)\n",
      "('non', 'af', '006') (100, 30, 30) (6, 30, 30) (6, 30, 30)\n",
      "('non', 'af', '007') (100, 30, 30) (4, 30, 30) (5, 30, 30)\n",
      "('non', 'af', '008') (100, 30, 30) (5, 30, 30) (6, 30, 30)\n",
      "('non', 'af', '009') (100, 30, 30) (3, 30, 30) (4, 30, 30)\n",
      "('non', 'af', '010') (100, 30, 30) (5, 30, 30) (5, 30, 30)\n",
      "('non', 'af', '011') (100, 30, 30) (5, 30, 30) (6, 30, 30)\n",
      "('non', 'af', '012') (100, 30, 30) (3, 30, 30) (4, 30, 30)\n",
      "('non', 'af', '013') (100, 30, 30) (6, 30, 30) (6, 30, 30)\n",
      "('non', 'af', '014') (25, 30, 30) (25, 30, 30)\n",
      "('non', 'af', '015') (22, 30, 30) (23, 30, 30)\n",
      "('non', 'af', '016') (28, 30, 30) (28, 30, 30)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.impute import KNNImputer\n",
    "from hrvanalysis.preprocessing import get_nn_intervals\n",
    "\n",
    "B = 100# 1000 10 cycle long samples for each subject\n",
    "series_size = 30\n",
    "fs = 125\n",
    "add_hrv = False\n",
    "add_morph = True\n",
    "\n",
    "annot_files = sorted(glob.glob(\"/home/meiri.yoav/biomed_proj/data/annotated/*\"))\n",
    "af_files = [f for f in annot_files if 'non' not in f]\n",
    "non_af_files = [f for f in annot_files if 'non' in f]\n",
    "\n",
    "names = [tuple(f.split('/')[-1].split('.')[0].split('_')[1:]) for f in annot_files]\n",
    "non_af_subs = [t  for t in list(names) if 'non' in t]\n",
    "af_subs = [t  for t in list(names) if 'non' not in t]\n",
    "\n",
    "train_subs = non_af_subs[:int(len(non_af_subs)*0.8)] + af_subs[:int(len(af_subs)*0.8)]\n",
    "test_subs = non_af_subs[int(len(non_af_subs)*0.8):] + af_subs[int(len(af_subs)*0.8):]\n",
    "\n",
    "# print how much af and non af subjects we have in train and test sets\n",
    "print(f'af subjects in train set: {len([s for s in train_subs if \"non\" not in s])}')\n",
    "print(f'non af subjects in train set: {len([s for s in train_subs if \"non\" in s])}')\n",
    "print(f'af subjects in test set: {len([s for s in test_subs if \"non\" not in s])}')\n",
    "print(f'non af subjects in test set: {len([s for s in test_subs if \"non\" in s])}')\n",
    "\n",
    "subs_features = {}\n",
    "for f in annot_files:\n",
    "    sub_name = tuple(f.split('/')[-1].split('.')[0].split('_')[1:])\n",
    "    # print(f'processing {sub_name}')\n",
    "    subs_features[sub_name] = {}\n",
    "    with open(f, 'rb') as f:\n",
    "        annot = pkl.load(f)\n",
    "\n",
    "    \n",
    "    X = annot['osignal_data']\n",
    "\n",
    "\n",
    "    if sub_name in train_subs:\n",
    "        # sample a random number between 0 and \n",
    "        # upper_bound = random.randint(0, int(len(X)/5))\n",
    "        test_subset = X.iloc[0:int(len(X)/5),:] # take the first 20% of the data as test subset\n",
    "        \n",
    "        first_index_of_X = int(len(X)/5)\n",
    "        # remove test subset from X\n",
    "        X = X.drop(test_subset.index)\n",
    "        \n",
    "        \n",
    "        signal = annot['hr']\n",
    "        peaks = annot['fiducials']['pk']\n",
    "        # # compute lower and upper bound that can be used for outlier removal\n",
    "        # lower_bound, upper_bound = get_outlier_bounds(raw_signal)\n",
    "        # signal = get_nn_intervals(annot['hr'], low_rri=lower_bound, high_rri=upper_bound)\n",
    "        \n",
    "        #? Setup test and validation sets\n",
    "        # devide test subset to 30 cycle long series and pad with zeros if needed\n",
    "        test_series = np.zeros((int(len(test_subset)/series_size), series_size, test_subset.shape[1]-3)) # substracting 3 because we ommit Tpi, sample_idx and os\n",
    "        # test_onsets = []\n",
    "        hrv_features = []\n",
    "        for i in range(int(len(test_subset)/series_size)):\n",
    "            # # put in test_onsets[i] the onset value of the record in idx[i]\n",
    "            # if i == int(len(test_subset)/series_size)-1:\n",
    "            #     test_onsets.append((test_subset.iloc[i*series_size,:].os, X.iloc[0,:].os)) # the ending of the last series is the beginning of the first series in X\n",
    "            # else:\n",
    "            #     test_onsets.append((test_subset.iloc[i*series_size,:].os, test_subset.iloc[(i+1)*series_size-1,:].os))\n",
    "            \n",
    "            # take peaks[i*series_size:(i+1)*series_size] and substract from each element it's previous element \n",
    "            if add_hrv:\n",
    "                right_bound = min((i+1)*series_size+1, len(peaks))\n",
    "                nn_intervals = np.diff(peaks[i*series_size : right_bound])\n",
    "                # print(i, ': ', nn_intervals.shape, excract_hrv_features(nn_intervals, fs).shape)\n",
    "                hrv_features.append(excract_hrv_features(nn_intervals, fs))\n",
    "            \n",
    "            if add_morph:\n",
    "                test_series[i,:,:] = test_subset.iloc[i*series_size:(i+1)*series_size,:].drop(columns=['Tpi', 'sample_idx', 'os'])\n",
    "        \n",
    "        # print(np.array(hrv_features).shape)\n",
    "        if add_hrv:\n",
    "            # add hrv features to test_series\n",
    "            hrv_features = np.array([hrv_features for i in range(series_size)]).reshape((len(hrv_features), series_size, hrv_features[0].shape[0]))\n",
    "            test_series = np.concatenate((test_series, hrv_features), axis=2)\n",
    "        \n",
    "        # if add_hrv:\n",
    "        #     # extract hrv features from test_series\n",
    "        #     test_hrv_features = np.nan_to_num(np.vstack(list(map(lambda x: excract_hrv_features(signal[int(x[0]):int(x[1])], fs), test_onsets))))\n",
    "        #     # add hrv features to test_series\n",
    "        #     test_hrv_features = np.array([test_hrv_features for i in range(series_size)]).reshape((test_hrv_features.shape[0], series_size, test_hrv_features.shape[1]))\n",
    "        #     test_series = np.concatenate((test_series, test_hrv_features), axis=2)\n",
    "\n",
    "        # split test_series to val split and test split and add them both to subs_features\n",
    "        subs_features[sub_name]['val_split'] = test_series[:int(len(test_series)/2)]\n",
    "        subs_features[sub_name]['test_split'] = test_series[int(len(test_series)/2):]\n",
    "        \n",
    "        #? Setup train set\n",
    "        \n",
    "        data, onsets, idx = sample_series(X, B, series_size)\n",
    "\n",
    "        if not add_morph:\n",
    "            # make data an array of zeros of the same shape as data\n",
    "            data = np.zeros(data.shape)\n",
    "        # print(data.shape)\n",
    "        if add_hrv:\n",
    "            hrv_features = []\n",
    "            for i in range(B):\n",
    "                right_bound = min(first_index_of_X+idx[i]+series_size+1, len(peaks))\n",
    "                nn_intervals = np.diff(peaks[first_index_of_X+idx[i] : right_bound])\n",
    "                hrv_features.append(excract_hrv_features(nn_intervals, fs))\n",
    "                \n",
    "            hrv_features = np.array([hrv_features for i in range(series_size)]).reshape((len(hrv_features), series_size, hrv_features[0].shape[0]))\n",
    "            data = np.concatenate((data, hrv_features), axis=2)\n",
    "            \n",
    "            # train_onsets = [(onsets[i], onsets[i+1]) if i < len(onsets)-1 else (onsets[i], len(signal)) for i in range(len(onsets)-1)]\n",
    "            # # extract hrv features from train_series\n",
    "            # train_hrv_features= np.nan_to_num(np.vstack(list(map(lambda x: excract_hrv_features(signal[int(x[0]):int(x[1])], fs), train_onsets))))\n",
    "            # # add hrv features to train_series\n",
    "            # train_hrv_features = np.array([train_hrv_features for i in range(series_size)]).reshape((train_hrv_features.shape[0], series_size, train_hrv_features.shape[1])) # to all series vectors add the same hrv features (because the characterize the whole record and not every beat)\n",
    "            # data = np.concatenate((data, train_hrv_features), axis=2)\n",
    "        \n",
    "        \n",
    "        subs_features[sub_name]['train_split'] = data\n",
    "    else:\n",
    "        # print('test_sub')\n",
    "        # In this case just split to 30 cycle long series and pad with zeros if needed\n",
    "        data = np.zeros((int(len(X)/series_size), series_size, X.shape[1]-3))\n",
    "        # test_onsets = []\n",
    "        hrv_features = []\n",
    "        for i in range(int(len(X)/series_size)):\n",
    "            # if i == int(len(test_subset)/series_size)-1:\n",
    "            #     test_onsets.append((X.iloc[i*series_size,:].os, len(signal)))\n",
    "            # else:\n",
    "            #     test_onsets.append((X.iloc[i*series_size,:].os, X.iloc[(i+1)*series_size-1,:].os))\n",
    "            \n",
    "            if add_hrv:\n",
    "                right_bound = min((i+1)*series_size+1, len(peaks))\n",
    "                if right_bound <= i*series_size:\n",
    "                    continue\n",
    "                # print((i*series_size, right_bound))\n",
    "                nn_intervals = np.diff(peaks[i*series_size : right_bound])\n",
    "                if len(nn_intervals) == 0:\n",
    "                    continue\n",
    "                hrv_features.append(excract_hrv_features(nn_intervals, fs))\n",
    "            if add_morph:\n",
    "                data[i,:,:] = X.iloc[i*series_size:(i+1)*series_size,:].drop(columns=['Tpi', 'sample_idx', 'os'])\n",
    "        \n",
    "        if add_hrv:\n",
    "            # add hrv features to test_series\n",
    "            hrv_features = np.array([hrv_features for i in range(series_size)]).reshape((len(hrv_features), series_size, hrv_features[0].shape[0]))\n",
    "            # print(hrv_features.shape)\n",
    "            data = np.concatenate((data[:len(hrv_features),:,:], hrv_features), axis=2)\n",
    "        \n",
    "        # if add_hrv:\n",
    "        #     # extract hrv features from test_series\n",
    "        #     test_hrv_features = np.nan_to_num(np.vstack(list(map(lambda x: excract_hrv_features(signal[int(x[0]):int(x[1])], fs), test_onsets))))\n",
    "        #     # add hrv features to test_series\n",
    "        #     test_hrv_features = np.array([test_hrv_features for i in range(series_size)]).reshape((test_hrv_features.shape[0],series_size, test_hrv_features.shape[1]))\n",
    "        #     data = np.concatenate((data, test_hrv_features), axis=2)\n",
    "        # split data to val split and test split and add them both to subs_features\n",
    "        subs_features[sub_name]['val_split'] = data[:int(len(data)/2)]\n",
    "        subs_features[sub_name]['test_split'] = data[int(len(data)/2):]\n",
    "        \n",
    "\n",
    "# print the name and the shapes of all subs in subs_features, and consider if it's a train sub or a test sub\n",
    "for sub in subs_features.keys():\n",
    "    if sub in train_subs:\n",
    "        print(sub, subs_features[sub]['train_split'].shape, subs_features[sub]['val_split'].shape, subs_features[sub]['test_split'].shape)\n",
    "    else:\n",
    "        print(sub, subs_features[sub]['val_split'].shape, subs_features[sub]['test_split'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('non', 'af', '001'), ('non', 'af', '002'), ('non', 'af', '003'), ('non', 'af', '005'), ('non', 'af', '006'), ('non', 'af', '007'), ('non', 'af', '008'), ('non', 'af', '009'), ('non', 'af', '010'), ('non', 'af', '011'), ('non', 'af', '012'), ('non', 'af', '013'), ('af', '001'), ('af', '002'), ('af', '003'), ('af', '005'), ('af', '006'), ('af', '007'), ('af', '008'), ('af', '009'), ('af', '010'), ('af', '011'), ('af', '012'), ('af', '013'), ('af', '014'), ('af', '015')]\n"
     ]
    }
   ],
   "source": [
    "print(train_subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (2600, 30, 30)\n",
      "y_train shape:  (2600,)\n",
      "X_val_from_train_subs shape:  (122, 30, 30)\n",
      "y_val_from_train_subs shape:  (122,)\n",
      "X_test_from_train_subs shape:  (135, 30, 30)\n",
      "y_test_from_train_subs shape:  (135,)\n",
      "X_val_from_test_subs shape:  (184, 30, 30)\n",
      "y_val_from_test_subs shape:  (184,)\n",
      "X_test_from_test_subs shape:  (187, 30, 30)\n",
      "y_test_from_test_subs shape:  (187,)\n",
      "train_subs:  [('non', 'af', '001'), ('non', 'af', '002'), ('non', 'af', '003'), ('non', 'af', '005'), ('non', 'af', '006'), ('non', 'af', '007'), ('non', 'af', '008'), ('non', 'af', '009'), ('non', 'af', '010'), ('non', 'af', '011'), ('non', 'af', '012'), ('non', 'af', '013'), ('af', '001'), ('af', '002'), ('af', '003'), ('af', '005'), ('af', '006'), ('af', '007'), ('af', '008'), ('af', '009'), ('af', '010'), ('af', '011'), ('af', '012'), ('af', '013'), ('af', '014'), ('af', '015')]\n",
      "test_subs:  [('non', 'af', '014'), ('non', 'af', '015'), ('non', 'af', '016'), ('af', '016'), ('af', '017'), ('af', '018'), ('af', '019')]\n"
     ]
    }
   ],
   "source": [
    "def create_dataset(sub_features):\n",
    "    # devide to train and test subjects where both train and test groups contain all the classes\n",
    "    \n",
    "    \n",
    "    X_train = np.concatenate([sub_features[t]['train_split'] for t in train_subs], axis=0) # amoung the train subjects, take 80% of each subject to train set\n",
    "    y_train = np.concatenate([np.ones(int(sub_features[t]['train_split'].shape[0]))*int('non' not in t) for t in train_subs], axis=0)\n",
    "    \n",
    "    X_val_from_train_subs = np.concatenate([sub_features[t]['val_split'] for t in train_subs], axis=0)\n",
    "    y_val_from_train_subs = np.concatenate([np.ones(int(sub_features[t]['val_split'].shape[0]))*int('non' not in t) for t in train_subs], axis=0)\n",
    "    # shuffle X_val_from_train_subs and y_val_from_train_subs togather\n",
    "    idx = np.random.permutation(X_val_from_train_subs.shape[0])\n",
    "    X_val_from_train_subs = X_val_from_train_subs[idx]\n",
    "    y_val_from_train_subs = y_val_from_train_subs[idx]\n",
    "    \n",
    "    X_test_from_train_subs = np.concatenate([sub_features[t]['test_split'] for t in train_subs], axis=0)\n",
    "    y_test_from_train_subs = np.concatenate([np.ones(int(sub_features[t]['test_split'].shape[0]))*int('non' not in t) for t in train_subs], axis=0)\n",
    "    # print(y_test_from_train_subs.shape, X_test_from_train_subs.shape)\n",
    "    # for i in range(len(y_test_from_train_subs)):\n",
    "    #     print(y_test_from_train_subs[i], X_test_from_train_subs[i].shape)\n",
    "    X_val_from_test_subs = np.concatenate([sub_features[t]['val_split'] for t in test_subs], axis=0)\n",
    "    y_val_from_test_subs = np.concatenate([np.ones(int(sub_features[t]['val_split'].shape[0]))*int('non' not in t) for t in test_subs], axis=0)\n",
    "    \n",
    "    X_test_from_test_subs = np.concatenate([sub_features[t]['test_split'] for t in test_subs], axis=0)\n",
    "    y_test_from_test_subs = np.concatenate([np.ones(int(sub_features[t]['test_split'].shape[0]))*int('non' not in t) for t in test_subs], axis=0)\n",
    "    \n",
    "    \n",
    "    return X_train, y_train, X_val_from_train_subs, y_val_from_train_subs, X_test_from_train_subs, y_test_from_train_subs, X_val_from_test_subs, y_val_from_test_subs, X_test_from_test_subs, y_test_from_test_subs, train_subs, test_subs\n",
    "    \n",
    "\n",
    "    \n",
    "X_train, y_train, X_val_from_train_subs, y_val_from_train_subs, X_test_from_train_subs, y_test_from_train_subs, X_val_from_test_subs, y_val_from_test_subs, X_test_from_test_subs, y_test_from_test_subs, train_subs, test_subs = create_dataset(subs_features)\n",
    "# print the shapes of the data\n",
    "print('X_train shape: ', X_train.shape)\n",
    "print('y_train shape: ', y_train.shape)\n",
    "print('X_val_from_train_subs shape: ', X_val_from_train_subs.shape)\n",
    "print('y_val_from_train_subs shape: ', y_val_from_train_subs.shape)\n",
    "print('X_test_from_train_subs shape: ', X_test_from_train_subs.shape)\n",
    "print('y_test_from_train_subs shape: ', y_test_from_train_subs.shape)\n",
    "print('X_val_from_test_subs shape: ', X_val_from_test_subs.shape)\n",
    "print('y_val_from_test_subs shape: ', y_val_from_test_subs.shape)\n",
    "print('X_test_from_test_subs shape: ', X_test_from_test_subs.shape)\n",
    "print('y_test_from_test_subs shape: ', y_test_from_test_subs.shape)\n",
    "\n",
    "print('train_subs: ', train_subs)\n",
    "print('test_subs: ', test_subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train nan:  False\n",
      "y_train nan:  False\n",
      "X_val_from_train_subs nan:  False\n",
      "y_val_from_train_subs nan:  False\n",
      "X_test_from_train_subs nan:  False\n",
      "y_test_from_train_subs nan:  False\n",
      "X_val_from_test_subs nan:  False\n",
      "y_val_from_test_subs nan:  False\n",
      "X_test_from_test_subs nan:  False\n",
      "y_test_from_test_subs nan:  False\n"
     ]
    }
   ],
   "source": [
    "# check if there is nan values in the data\n",
    "print('X_train nan: ', np.isnan(X_train).any())\n",
    "print('y_train nan: ', np.isnan(y_train).any())\n",
    "print('X_val_from_train_subs nan: ', np.isnan(X_val_from_train_subs).any())\n",
    "print('y_val_from_train_subs nan: ', np.isnan(y_val_from_train_subs).any())\n",
    "print('X_test_from_train_subs nan: ', np.isnan(X_test_from_train_subs).any())\n",
    "print('y_test_from_train_subs nan: ', np.isnan(y_test_from_train_subs).any())\n",
    "print('X_val_from_test_subs nan: ', np.isnan(X_val_from_test_subs).any())\n",
    "print('y_val_from_test_subs nan: ', np.isnan(y_val_from_test_subs).any())\n",
    "print('X_test_from_test_subs nan: ', np.isnan(X_test_from_test_subs).any())\n",
    "print('y_test_from_test_subs nan: ', np.isnan(y_test_from_test_subs).any())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/meiri.yoav/biomed_proj/wandb/run-20230610_193055-ba1g7dji</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yoavmeiri/bioML%20-%20AF%20detection%20from%20PPG%20-%20lstm%20baseline/runs/ba1g7dji' target=\"_blank\">only Morph, B=100</a></strong> to <a href='https://wandb.ai/yoavmeiri/bioML%20-%20AF%20detection%20from%20PPG%20-%20lstm%20baseline' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yoavmeiri/bioML%20-%20AF%20detection%20from%20PPG%20-%20lstm%20baseline' target=\"_blank\">https://wandb.ai/yoavmeiri/bioML%20-%20AF%20detection%20from%20PPG%20-%20lstm%20baseline</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yoavmeiri/bioML%20-%20AF%20detection%20from%20PPG%20-%20lstm%20baseline/runs/ba1g7dji' target=\"_blank\">https://wandb.ai/yoavmeiri/bioML%20-%20AF%20detection%20from%20PPG%20-%20lstm%20baseline/runs/ba1g7dji</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "user = \"YoavMeiri\"\n",
    "project = \"bioML - AF detection from PPG - lstm baseline\"\n",
    "name = \"only Morph, B=100\"\n",
    "run = wandb.init(entity=user, project=project, name=name, config={\n",
    "    \"architecture\": 'biderctional lstm',\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/meiri.yoav/mambaforge/envs/bioml/lib/python3.11/site-packages/pytorch_lightning/loggers/wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/meiri.yoav/mambaforge/envs/bioml/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:176: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=2)`.\n",
      "  rank_zero_warn(\n",
      "/home/meiri.yoav/mambaforge/envs/bioml/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95effeb5b69944b8a46241aac82c4dbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/meiri.yoav/mambaforge/envs/bioml/lib/python3.11/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">  Runningstage.validating  </span>┃<span style=\"font-weight: bold\">                           </span>┃<span style=\"font-weight: bold\">                           </span>┃\n",
       "┃<span style=\"font-weight: bold\">          metric           </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃<span style=\"font-weight: bold\">       DataLoader 1        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     val_test_subs_acc     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">                           </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.40760868787765503    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     val_test_subs_f1      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">                           </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.40760868787765503    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    val_test_subs_loss     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">                           </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6975266933441162     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    val_train_subs_acc     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.4590163826942444     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">                           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     val_train_subs_f1     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.4590163826942444     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">                           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    val_train_subs_loss    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6955657601356506     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">                           </span>│\n",
       "└───────────────────────────┴───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m Runningstage.validating \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m                           \u001b[0m┃\u001b[1m                           \u001b[0m┃\n",
       "┃\u001b[1m \u001b[0m\u001b[1m         metric          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 1       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m    val_test_subs_acc    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m                         \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.40760868787765503   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    val_test_subs_f1     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m                         \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.40760868787765503   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m   val_test_subs_loss    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m                         \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6975266933441162    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m   val_train_subs_acc    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.4590163826942444    \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m                         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    val_train_subs_f1    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.4590163826942444    \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m                         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m   val_train_subs_loss   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6955657601356506    \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m                         \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name     | Type               | Params\n",
      "------------------------------------------------\n",
      "0 | lstm     | LSTM               | 148 K \n",
      "1 | fc       | Sequential         | 8.4 K \n",
      "2 | accuracy | MulticlassAccuracy | 0     \n",
      "3 | f1       | MulticlassF1Score  | 0     \n",
      "4 | auc      | MulticlassAUROC    | 0     \n",
      "------------------------------------------------\n",
      "156 K     Trainable params\n",
      "0         Non-trainable params\n",
      "156 K     Total params\n",
      "0.627     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "226c6f93ce6c4e0bb1632edeec9feaac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/meiri.yoav/mambaforge/envs/bioml/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "639e4da47a2043428d680613bce3f047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aec317eb39ed4792b8324acf18ec9f5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "074c89f4054d4edc8366e6b4c2936d86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efdbd3ab347545a4a8c6063c0e6d882e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10515e8939304370a8aa257b61933462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b6a9f3ed010463f88e950a600caedfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03c2c31494544edabe79a939c373e34b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6e2571ce3ce4f72a80d58ebbbf9fb4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74df45b091724d56a2fc021b7f306c40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62326400c3e945d4aaa485160cd0f036",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2288fbb760ef491a9d563b837aa881f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n",
      "/home/meiri.yoav/mambaforge/envs/bioml/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46549cef879d47608e1fdcd7de7a8361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">   Runningstage.testing    </span>┃<span style=\"font-weight: bold\">                           </span>┃<span style=\"font-weight: bold\">                           </span>┃\n",
       "┃<span style=\"font-weight: bold\">          metric           </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃<span style=\"font-weight: bold\">       DataLoader 1        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    test_test_subs_acc     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">                           </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7647058963775635     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     test_test_subs_f1     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">                           </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7647058963775635     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    test_test_subs_loss    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">                           </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5390118360519409     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    test_train_subs_acc    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8666666746139526     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">                           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    test_train_subs_f1     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8666666746139526     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">                           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">   test_train_subs_loss    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.4334997534751892     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">                           </span>│\n",
       "└───────────────────────────┴───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m  Runningstage.testing   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m                           \u001b[0m┃\u001b[1m                           \u001b[0m┃\n",
       "┃\u001b[1m \u001b[0m\u001b[1m         metric          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 1       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m   test_test_subs_acc    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m                         \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7647058963775635    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    test_test_subs_f1    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m                         \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7647058963775635    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m   test_test_subs_loss   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m                         \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5390118360519409    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m   test_train_subs_acc   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8666666746139526    \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m                         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m   test_train_subs_f1    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8666666746139526    \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m                         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m  test_train_subs_loss   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.4334997534751892    \u001b[0m\u001b[35m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m                         \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12074be61ed84d3c9fc1c34972c905f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.027 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.124185…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇█</td></tr><tr><td>test_test_subs_acc/dataloader_idx_1</td><td>▁</td></tr><tr><td>test_test_subs_f1/dataloader_idx_1</td><td>▁</td></tr><tr><td>test_test_subs_loss/dataloader_idx_1</td><td>▁</td></tr><tr><td>test_train_subs_acc/dataloader_idx_0</td><td>▁</td></tr><tr><td>test_train_subs_f1/dataloader_idx_0</td><td>▁</td></tr><tr><td>test_train_subs_loss/dataloader_idx_0</td><td>▁</td></tr><tr><td>train_acc</td><td>▁▆▇▇▇▇▇███</td></tr><tr><td>train_f1</td><td>▁▆▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▅▃▂▂▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▂▂▂▂▃▃▄▄▄▄▅▅▆▆▇▇▇▇███</td></tr><tr><td>val_test_subs_acc/dataloader_idx_1</td><td>▁▇█▇█▇███▇█</td></tr><tr><td>val_test_subs_f1/dataloader_idx_1</td><td>▁▇█▇█▇███▇█</td></tr><tr><td>val_test_subs_loss/dataloader_idx_1</td><td>█▆▂▂▂▂▁▁▂▂▂</td></tr><tr><td>val_train_subs_acc/dataloader_idx_0</td><td>▁▇▇▇███████</td></tr><tr><td>val_train_subs_f1/dataloader_idx_0</td><td>▁▇▇▇███████</td></tr><tr><td>val_train_subs_loss/dataloader_idx_0</td><td>█▇▃▂▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>test_test_subs_acc/dataloader_idx_1</td><td>0.76471</td></tr><tr><td>test_test_subs_f1/dataloader_idx_1</td><td>0.76471</td></tr><tr><td>test_test_subs_loss/dataloader_idx_1</td><td>0.53901</td></tr><tr><td>test_train_subs_acc/dataloader_idx_0</td><td>0.86667</td></tr><tr><td>test_train_subs_f1/dataloader_idx_0</td><td>0.86667</td></tr><tr><td>test_train_subs_loss/dataloader_idx_0</td><td>0.4335</td></tr><tr><td>train_acc</td><td>0.91308</td></tr><tr><td>train_f1</td><td>0.91308</td></tr><tr><td>train_loss</td><td>0.39974</td></tr><tr><td>trainer/global_step</td><td>820</td></tr><tr><td>val_test_subs_acc/dataloader_idx_1</td><td>0.71739</td></tr><tr><td>val_test_subs_f1/dataloader_idx_1</td><td>0.71739</td></tr><tr><td>val_test_subs_loss/dataloader_idx_1</td><td>0.59234</td></tr><tr><td>val_train_subs_acc/dataloader_idx_0</td><td>0.78689</td></tr><tr><td>val_train_subs_f1/dataloader_idx_0</td><td>0.78689</td></tr><tr><td>val_train_subs_loss/dataloader_idx_0</td><td>0.512</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">only Morph, B=100</strong> at: <a href='https://wandb.ai/yoavmeiri/bioML%20-%20AF%20detection%20from%20PPG%20-%20lstm%20baseline/runs/ba1g7dji' target=\"_blank\">https://wandb.ai/yoavmeiri/bioML%20-%20AF%20detection%20from%20PPG%20-%20lstm%20baseline/runs/ba1g7dji</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230610_193055-ba1g7dji/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a dataset and model using pytorch lightning module and biderectional lstm for this task\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import classification_report\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import torchmetrics\n",
    "\n",
    "wandb_logger = WandbLogger()\n",
    "\n",
    "\n",
    "class afPPGDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y\n",
    "        self.len = len(y)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "class afPPGModel(pl.LightningModule):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, learning_rate):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "        # Add 2 layer fully-connected with ReLu in the middle\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size*2, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_classes),\n",
    "            nn.Softmax())\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.f1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.auc = torchmetrics.AUROC(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._log_stats(batch,  name_template='train')\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx, dataloader_idx):\n",
    "        if dataloader_idx == 0:\n",
    "            loss = self._log_stats(batch,  name_template='val_train_subs')\n",
    "\n",
    "        elif dataloader_idx == 1:\n",
    "            loss = self._log_stats(batch,  name_template='val_test_subs')\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx, dataloader_idx):\n",
    "        if dataloader_idx == 0:\n",
    "            loss = self._log_stats(batch,  name_template='test_train_subs')\n",
    "        else:\n",
    "            loss = self._log_stats(batch,  name_template='test_test_subs')\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        train_dataset = afPPGDataset(X_train, y_train)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        return train_loader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        val_from_train_subs_dataset = afPPGDataset(X_val_from_train_subs, y_val_from_train_subs)\n",
    "        val_from_train_subs_loader = DataLoader(val_from_train_subs_dataset, batch_size=32, shuffle=False)\n",
    "        \n",
    "        val_from_test_subs_dataset = afPPGDataset(X_val_from_test_subs, y_val_from_test_subs)\n",
    "        val_from_test_subs_loader = DataLoader(val_from_test_subs_dataset, batch_size=32, shuffle=False)\n",
    "        return [val_from_train_subs_loader, val_from_test_subs_loader]\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        test_from_train_subs_dataset = afPPGDataset(X_test_from_train_subs, y_test_from_train_subs)\n",
    "        test_from_train_subs_loader = DataLoader(test_from_train_subs_dataset, batch_size=32, shuffle=False)\n",
    "        \n",
    "        test_from_test_subs_dataset = afPPGDataset(X_test_from_test_subs, y_test_from_test_subs)\n",
    "        test_from_test_subs_loader = DataLoader(test_from_test_subs_dataset, batch_size=32, shuffle=False)\n",
    "        return [test_from_train_subs_loader, test_from_test_subs_loader]\n",
    "    \n",
    "    def _log_stats(self, batch, name_template):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        loss = F.cross_entropy(logits, y.long())\n",
    "        f1 = self.f1(preds, y)\n",
    "        acc = self.accuracy(preds, y)\n",
    "        \n",
    "        self.log(name_template + '_loss', loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(name_template + '_acc', acc, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(name_template + '_f1', f1, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def predict(self, x):\n",
    "        y_hat = self(x)\n",
    "        return y_hat\n",
    "    \n",
    "    def predict_proba(self, x):\n",
    "        y_hat = self(x)\n",
    "        return F.softmax(y_hat, dim=1)\n",
    "\n",
    "# create a model\n",
    "input_dim = 59 if add_hrv else 30\n",
    "model = afPPGModel(input_size=input_dim, hidden_size=64, num_layers=2, num_classes=2, learning_rate=0.0001)\n",
    "\n",
    "# create a trainer\n",
    "# trainer = pl.Trainer(max_epochs=15)\n",
    "trainer = pl.Trainer(max_epochs=10, logger=wandb_logger, num_sanity_val_steps=2, accelerator='cpu')\n",
    "\n",
    "trainer.validate(model)\n",
    "# train the model\n",
    "trainer.fit(model)\n",
    "\n",
    "# test the model\n",
    "trainer.test(model)\n",
    "\n",
    "wandb.finish()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
